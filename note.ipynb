{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from google.cloud import storage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"development-416403-b306695feb67.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.006605386734008789\n"
     ]
    }
   ],
   "source": [
    "# open file and \n",
    "start_time = time.time()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "files = open('llm.txt', 'r')\n",
    "\n",
    "text = files.read()\n",
    "\n",
    "# chunk to word\n",
    "words = [word.lower() for word in text.split() if word.isalpha()]\n",
    "\n",
    "# remove the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "without_stop_word_file = [word for word in words if word not in stop_words]\n",
    "\n",
    "\n",
    "#  store the most commom word in list\n",
    "most_common_word_list = []\n",
    "\n",
    "for most_common_word, _ in FreqDist(without_stop_word_file).most_common(10):\n",
    "    most_common_word_list.append(most_common_word)\n",
    "\n",
    "    # vectorize the list of the most common word\n",
    "tfid = TfidfVectorizer()\n",
    "text_vector =tfid.fit_transform(most_common_word_list)\n",
    "# print(tfid.get_feature_names_out())\n",
    "# print(text_vector.todense())\n",
    "\n",
    "# time taken for execution\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"time taken:\",elapsed_time)\n",
    "# time taken: 0.006171464920043945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch file from the bucket\n",
    "\n",
    "def fetch_files ():\n",
    "    star_fn = time.time()\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = \"development_awarri\"\n",
    "    prefix = \"read/english/src-docs\"\n",
    "    source_bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=None)\n",
    "    for blob in blobs:\n",
    "        file = blob.name\n",
    "        blob_file = source_bucket.blob(file)\n",
    "        with blob_file.open(\"r\") as f:\n",
    "            text = f.read()\n",
    "            print(text)\n",
    "    end_fn = time.time()\n",
    "    print(\"fn time:\",end_fn-star_fn)\n",
    "fetch_files()   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
